{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nlpaug","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:39:12.332980Z","iopub.execute_input":"2021-11-08T15:39:12.333334Z","iopub.status.idle":"2021-11-08T15:39:22.768981Z","shell.execute_reply.started":"2021-11-08T15:39:12.333224Z","shell.execute_reply":"2021-11-08T15:39:22.768114Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw  \nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf_train = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='latin_1')\ndf_train.isnull().sum()\n\nprint(df_train.isnull().sum())\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:39:22.770813Z","iopub.execute_input":"2021-11-08T15:39:22.771085Z","iopub.status.idle":"2021-11-08T15:39:32.751480Z","shell.execute_reply.started":"2021-11-08T15:39:22.771050Z","shell.execute_reply":"2021-11-08T15:39:32.750615Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\n\nfig = plt.figure(figsize=(16,8))\nsns.histplot(df_train['Sentiment'], color='black', stat='density')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:39:32.752927Z","iopub.execute_input":"2021-11-08T15:39:32.754428Z","iopub.status.idle":"2021-11-08T15:39:33.148852Z","shell.execute_reply.started":"2021-11-08T15:39:32.754388Z","shell.execute_reply":"2021-11-08T15:39:33.148161Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display\n\nprint(df_train['Sentiment'].value_counts())\nky = df_train.groupby('Location')['Sentiment'].agg('value_counts') ## Count when used in agg functionn  gives all the vals but value_counts give sep label vals \n\ndisplay(df_train.head())\n\nprint(df_train.isnull().sum())\n\ncountry = ['India','Australia','UK','USA']\ntitle = ['India / Australia_Distrib.', 'UK / USA_Distrib.']\nfig, axs = plt.subplots(2,1, figsize=(12,8))\nind = 0\nfor i in range(2):\n    axs[i].set_title(title[i])\n    axs[i].bar(x = ['P', 'N', 'Neu', 'Ext. P', 'Ext. N'], height = ky[country[ind]].tolist(), label = country[ind])\n    axs[i].bar(x = ['P', 'N', 'Neu', 'Ext. P', 'Ext. N'], height = ky[country[ind+1]].tolist(), bottom=True, label = country[ind+1])\n    ind+=2\n    axs[i].legend()\n\nplt.show()\n\ndf_train = df_train.fillna(method = 'ffill', axis=0) ## only the null vals are forward-filled in this case not the entire row itself\ndf_train = df_train.dropna()\nprint(df_train['Sentiment'].value_counts())\ndisplay(df_train.head())\nprint(df_train.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:39:33.151160Z","iopub.execute_input":"2021-11-08T15:39:33.151488Z","iopub.status.idle":"2021-11-08T15:39:33.715684Z","shell.execute_reply.started":"2021-11-08T15:39:33.151447Z","shell.execute_reply":"2021-11-08T15:39:33.714941Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## Non-Contextual Word-Embedding\naug_w2v = naw.WordEmbsAug(\n    model_type='glove', model_path='../input/glove6b/glove.6B.200d.txt',\n    action=\"substitute\")\naug_w2v.aug_p=0.2\n\ntext = 'I am aLik' ## random_synonym_replacement, random_insertion etc \nprint(text)\nprint(aug_w2v.augment(text))","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:39:33.716960Z","iopub.execute_input":"2021-11-08T15:39:33.717624Z","iopub.status.idle":"2021-11-08T15:41:50.790076Z","shell.execute_reply.started":"2021-11-08T15:39:33.717582Z","shell.execute_reply":"2021-11-08T15:41:50.788217Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"'''Do not use Augmentation for Cross-Validation, so first-split then augment'''\nimport random \nfrom pprint import pprint\n\nall_vals = df_train['Sentiment'].value_counts()\nall_sen = all_vals.keys().tolist()[1:]\ntarget_val  = int(all_vals.iloc[0])\n\ndef augment_text(min_df_train, n_samples = 30 , pr=0.2):\n    aug_data = min_df_train.sample(n = n_samples)\n    text_check = []\n    for i in range(n_samples):\n        rnd = random.randint(0, min_df_train.shape[0])\n        rnd_text = min_df_train['OriginalTweet'].iloc[rnd]\n        aug_text = aug_w2v.augment(rnd_text)\n        aug_data['OriginalTweet'].iloc[i] = aug_text\n        text_check.append(aug_text)\n        \n    return aug_data, text_check\n\n\nfor step, min_sen in enumerate(all_sen):\n    \n    print('Min_Sentiment_taken: ', min_sen)\n    min_df_train = df_train[df_train.Sentiment == min_sen]\n    \n    '''curr_val = int(all_vals[min_sen]) // n_samples = target_val - curr_val''' ## for the sake of complete majority balancing \n    aug_data, text_check = augment_text(min_df_train, n_samples = 100)\n\n    print('Before_Augmentation: ',df_train.shape)\n    if step == 0:\n        df_train_new = pd.concat([df_train, aug_data], axis = 0)\n    else:\n        df_train_new = pd.concat([df_train_new, aug_data], axis=0)\n\n    print('After_Augmentation: ', df_train_new.shape)\n    print('Few_aug_samples: ')\n    print()\n    print(text_check[0][:70], '...')\n    print(text_check[1][:70], '...')\n    print('**'*50)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:41:50.791636Z","iopub.execute_input":"2021-11-08T15:41:50.791936Z","iopub.status.idle":"2021-11-08T15:44:16.737459Z","shell.execute_reply.started":"2021-11-08T15:41:50.791897Z","shell.execute_reply":"2021-11-08T15:44:16.736688Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = df_train_new.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:44:16.738920Z","iopub.execute_input":"2021-11-08T15:44:16.739367Z","iopub.status.idle":"2021-11-08T15:44:16.757698Z","shell.execute_reply.started":"2021-11-08T15:44:16.739327Z","shell.execute_reply":"2021-11-08T15:44:16.756812Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## Checking _stochasticity\n\nprint('df_train_shape: ', df.shape)\nprint('UserName_stochasticity: ',len(df['UserName'].unique().tolist()))\nprint('ScreenName_stochasticity: ',len(df['ScreenName'].unique().tolist()))\nprint('Location_stoch: ',len(df['Location'].unique().tolist()))\n\ndf_train_new = df.drop(['UserName','ScreenName'], axis=1)\nprint(df_train_new.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:44:16.761953Z","iopub.execute_input":"2021-11-08T15:44:16.762326Z","iopub.status.idle":"2021-11-08T15:44:16.799081Z","shell.execute_reply.started":"2021-11-08T15:44:16.762285Z","shell.execute_reply":"2021-11-08T15:44:16.798300Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \n\n#'''\nrem = '-'\nk = df_train_new['TweetAt'].tolist()\nk = '~~~'.join(k).translate(str.maketrans('', '', rem))\nk = k.split('~~~')\ndf_train_new['TweetAt'] = k\n#'''\n\ntweet_std = StandardScaler().fit_transform(np.array(df_train_new['TweetAt'].tolist()).reshape(-1,1))\ntweet_std = np.ravel(tweet_std).tolist()\ndf_train_new['TweetAt'] = tweet_std\ndf_train_new.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:44:16.800226Z","iopub.execute_input":"2021-11-08T15:44:16.800666Z","iopub.status.idle":"2021-11-08T15:44:16.917850Z","shell.execute_reply.started":"2021-11-08T15:44:16.800630Z","shell.execute_reply":"2021-11-08T15:44:16.916999Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import string\n\nrem = string.punctuation\nrem = rem.replace('~','')\nprint('rem_punc: ', rem)\n\nloc_list = df_train_new.Location.tolist()\nprint(loc_list[:4])\nloc_str = ' ~~~'.join(loc_list).lower().translate(str.maketrans(\"\",\"\",rem))\nloc_all_list = loc_str.split('~~~')\nloc_list = list(set(loc_all_list))\nprint(loc_list[:4])\nprint('Loc_List_Length: ',len(loc_list))\n\ninv_loc_dic = dict(enumerate(loc_list))\nloc_dic = {val:key for key, val in inv_loc_dic.items()}\n\ninv_label_dic = dict(enumerate(df_train_new.Sentiment.unique().tolist()))\nlabel_dic = {val:key for key, val in inv_label_dic.items()}\nprint(label_dic)\n\ndf_train_new.Location = loc_all_list\ndf_train_new.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:44:16.921313Z","iopub.execute_input":"2021-11-08T15:44:16.921584Z","iopub.status.idle":"2021-11-08T15:44:17.029738Z","shell.execute_reply.started":"2021-11-08T15:44:16.921548Z","shell.execute_reply":"2021-11-08T15:44:17.028790Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.histplot(df_train_new['Sentiment'], kde=False, color= 'black', stat = 'count')\nplt.show()\n\ndf_train_new.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:44:17.031535Z","iopub.execute_input":"2021-11-08T15:44:17.031803Z","iopub.status.idle":"2021-11-08T15:44:17.351085Z","shell.execute_reply.started":"2021-11-08T15:44:17.031766Z","shell.execute_reply":"2021-11-08T15:44:17.350338Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for step, (loc, label) in enumerate(zip(df_train_new.Location, df_train_new.Sentiment)):\n    df_train_new['Location'].iloc[step] = loc_dic.get(loc)\n    df_train_new['Sentiment'].iloc[step] = label_dic.get(label)\n\ndf_train_new.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:44:17.352493Z","iopub.execute_input":"2021-11-08T15:44:17.353361Z","iopub.status.idle":"2021-11-08T15:44:53.106662Z","shell.execute_reply.started":"2021-11-08T15:44:17.353319Z","shell.execute_reply":"2021-11-08T15:44:53.104346Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\nmaxx = len(df_train_new.OriginalTweet.iloc[0].split(' '))\nfor i in df_train_new.OriginalTweet.tolist():\n    if len(i.split(' ')) > maxx:\n        maxx = len(i.split(' '))\nmax_seq_len = maxx\nprint('Max_seq_length: ',max_seq_len)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:07:11.792093Z","iopub.execute_input":"2021-11-08T16:07:11.792380Z","iopub.status.idle":"2021-11-08T16:07:11.893846Z","shell.execute_reply.started":"2021-11-08T16:07:11.792349Z","shell.execute_reply":"2021-11-08T16:07:11.892958Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\ndata_x = df_train_new[['Location', 'TweetAt', 'OriginalTweet']].to_numpy()\ndata_y = df_train_new['Sentiment'].to_numpy()\nprint(data_x.shape, data_y.shape)\n\nx_train, x_test, y_train, y_test = train_test_split(data_x, data_y, random_state=28)\nprint(x_train.shape, x_test.shape)\ny_train = y_train.astype('int32')\nx_train_text = x_train[:,2]\nx_train_num = np.delete(x_train, 2, axis = 1)\nx_test_text = x_test[:,2]\nx_test_num = np.delete(x_test, 2, axis = 1)\n\n## you can explictly pass the Vocabulary_Data\ntweet_vect = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_seq_len)#vocabulary=vocab_data \ntweet_vect.adapt(x_train_text)\ninv_tweet_dic = dict(enumerate(tweet_vect.get_vocabulary()))\ntweet_dic = {value:key for key, value in inv_tweet_dic.items()}\nprint('Tweet_Dic: ',len(list(tweet_dic)))\n\nprint(x_train_text.shape, x_train_num.shape, x_test_text.shape, x_test_num.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:07:15.646064Z","iopub.execute_input":"2021-11-08T16:07:15.646504Z","iopub.status.idle":"2021-11-08T16:07:23.045899Z","shell.execute_reply.started":"2021-11-08T16:07:15.646468Z","shell.execute_reply":"2021-11-08T16:07:23.045089Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import class_weight\nfrom pprint import pprint\n\nclass_weights = class_weight.compute_sample_weight('balanced', np.unique(y_train), y_train)\nclass_weights_dict = dict(enumerate(class_weights))\n\npprint(class_weights_dict)\n\nsample_weights = np.ones(shape = (y_train.shape[0],))\nfor i in range(5):\n    sample_weights[y_train == i] = class_weights_dict.get(i)\nprint(sample_weights.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:07:23.047699Z","iopub.execute_input":"2021-11-08T16:07:23.047957Z","iopub.status.idle":"2021-11-08T16:07:23.068402Z","shell.execute_reply.started":"2021-11-08T16:07:23.047922Z","shell.execute_reply":"2021-11-08T16:07:23.067583Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## Model_Creation\nx_train_num = x_train_num.astype('float32')\nnorm_layer = tf.keras.layers.Normalization(axis=1)\nnorm_layer.adapt(x_train_num)\n\ninp_text = tf.keras.layers.Input(shape = (), name='text_input', dtype=tf.string) ## while use vectorization layer change the dtype to tf.string\nvect_text = tweet_vect(inp_text)\nembd_text = tf.keras.layers.Embedding(output_dim = 1024, input_dim = len(list(tweet_dic)))(vect_text)\ninp_num = tf.keras.layers.Input(shape = (x_train_num.shape[-1],), name='Num_text')\nnorm_num = norm_layer(inp_num)\ngru_1 = tf.keras.layers.GRU(748, return_sequences=True, recurrent_dropout=0.5, dropout=0.3)(embd_text)\ngru_2 = tf.keras.layers.GRU(512, return_sequences=True, recurrent_dropout=0.5)(gru_1)\ngru_3 = tf.keras.layers.GRU(512)(gru_2)\nnum_1 = tf.keras.layers.Dense(512)(norm_num)\nnum_1 = tf.keras.layers.BatchNormalization()(num_1)\nnum_1 = tf.keras.layers.Activation('tanh')(num_1)\nnum_1 = tf.keras.layers.Dropout(0.4)(num_1)\nnum_2 =tf.keras.layers.Dense(512)(num_1)\nnum_2 = tf.keras.layers.BatchNormalization()(num_2)\nnum_2 = tf.keras.layers.Activation('tanh')(num_2)\n\n\nconc_layer = tf.keras.layers.Concatenate(axis=-1)([gru_3, num_2])\nf_dense = tf.keras.layers.Dense(len(list(label_dic)), activation = 'softmax')(conc_layer)\n\nnaive_model = tf.keras.models.Model(inputs = [inp_text, inp_num], outputs = f_dense, name='naive_model')\nnaive_model.compile(loss = 'sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3), metrics=['accuracy'])\n\ntf.keras.utils.plot_model(naive_model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:27:27.576973Z","iopub.execute_input":"2021-11-08T16:27:27.577649Z","iopub.status.idle":"2021-11-08T16:27:30.180985Z","shell.execute_reply.started":"2021-11-08T16:27:27.577590Z","shell.execute_reply":"2021-11-08T16:27:30.180153Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"## calbacks \n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10)\n\ndef scheduler(epoch, learning_rate):\n    if epoch <= 4:\n        return learning_rate\n    else:\n        return learning_rate * 0.01\n\nlrs = tf.keras.callbacks.LearningRateScheduler(scheduler)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:27:46.917346Z","iopub.execute_input":"2021-11-08T16:27:46.918172Z","iopub.status.idle":"2021-11-08T16:27:46.923217Z","shell.execute_reply.started":"2021-11-08T16:27:46.918119Z","shell.execute_reply":"2021-11-08T16:27:46.922459Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.astype('float32')\n\nhis = naive_model.fit(x = [x_train_text, x_train_num],y = y_train, batch_size = 32, epochs = 1, validation_split=0.2, callbacks=[stop_early, lrs] , sample_weight = sample_weights)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:27:50.798369Z","iopub.execute_input":"2021-11-08T16:27:50.798931Z","iopub.status.idle":"2021-11-08T16:41:22.360497Z","shell.execute_reply.started":"2021-11-08T16:27:50.798891Z","shell.execute_reply":"2021-11-08T16:41:22.359723Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nx_test_num = x_test_num.astype('float32')\nx_test = [x_test_text, x_test_num]\ny_test = y_test.astype('float32')\n#naive_model.evaluate(x_test, y_test)\n\nypred = np.argmax(naive_model.predict(x_test), axis=-1)\nprint()\nprint('Confuion_matrix: ')\nprint(confusion_matrix(y_test, ypred))","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:41:22.362424Z","iopub.execute_input":"2021-11-08T16:41:22.362850Z","iopub.status.idle":"2021-11-08T16:41:51.772766Z","shell.execute_reply.started":"2021-11-08T16:41:22.362810Z","shell.execute_reply":"2021-11-08T16:41:51.771948Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\nimport random\n\ndef predict(seed):\n    print('Prediction: ')\n    print()\n    yhat = np.argmax(naive_model([np.array([x_train_text[seed]]).reshape(1,-1), x_train_num[seed].reshape(1,-1)]), axis=-1)\n    print('Input_Sentence: ')\n    print()\n    pprint(x_train_text[seed])\n    print()\n    print('Actual_Result: ')\n    print(inv_label_dic.get(y_train[seed]))\n    print()\n    print('Predicted_Result: ')\n    print(inv_label_dic.get(yhat[-1]))\n\n\nfor _ in range(5):\n    seed = random.randint(0, x_train_text.shape[0])\n    predict(seed)\n    print('**' *50)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T16:41:51.774110Z","iopub.execute_input":"2021-11-08T16:41:51.774377Z","iopub.status.idle":"2021-11-08T16:41:57.261850Z","shell.execute_reply.started":"2021-11-08T16:41:51.774342Z","shell.execute_reply":"2021-11-08T16:41:57.261151Z"},"trusted":true},"execution_count":26,"outputs":[]}]}